{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39820c5a-5e3b-4b4c-b659-e86f0ab3ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"data/updatemergedata.csv\")\n",
    "data.head()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Level'] = label_encoder.fit_transform(data['Level']) \n",
    "data['Price'] = data['Price'].map({'Berbayar': 1, 'Gratis': 0})\n",
    "\n",
    "# 1. Gabungkan fitur teks menjadi satu kolom\n",
    "indonesian_stopwords = stopwords.words('indonesian')\n",
    "numerical_features = data[['Level', 'Price']].values\n",
    "data['Combined Summary'] = data['Learning Path'] + ' ' + data['Learning Path Summary'] + ' ' + data['Course Name_x'] + ' ' + data['Course Summary']\n",
    "# 2. TF-IDF untuk fitur teks\n",
    "tfidf = TfidfVectorizer(stop_words=indonesian_stopwords)\n",
    "tfidf_matrix = tfidf.fit_transform(data['Combined Summary']).toarray()\n",
    "\n",
    "# 3. Scaling data untuk fitur numerik jika diperlukan\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(tfidf_matrix)\n",
    "X_combined = np.hstack((X_scaled, numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc9e07b-deef-443e-92b3-2a9ad140d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (6039, 409)\n",
      "Shape of numerical features: (6039, 2)\n",
      "Shape of combined data: (6039, 411)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)\n",
    "print(\"Shape of numerical features:\", numerical_features.shape)\n",
    "print(\"Shape of combined data:\", X_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9855e03b-8a04-4688-82aa-54151a84596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 2.1020 - val_accuracy: 0.0000e+00 - val_loss: 2.1260\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.0855 - val_accuracy: 0.0000e+00 - val_loss: 1.8383\n",
      "Epoch 3/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7863 - val_accuracy: 0.0000e+00 - val_loss: 1.7602\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7048 - val_accuracy: 0.0000e+00 - val_loss: 1.7387\n",
      "Epoch 5/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6758 - val_accuracy: 0.0000e+00 - val_loss: 1.7250\n",
      "Epoch 6/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6620 - val_accuracy: 0.0000e+00 - val_loss: 1.7163\n",
      "Epoch 7/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6556 - val_accuracy: 0.0000e+00 - val_loss: 1.7101\n",
      "Epoch 8/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6466 - val_accuracy: 0.0000e+00 - val_loss: 1.7034\n",
      "Epoch 9/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6425 - val_accuracy: 0.0000e+00 - val_loss: 1.7017\n",
      "Epoch 10/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6437 - val_accuracy: 0.0000e+00 - val_loss: 1.6964\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6398 - val_accuracy: 0.0000e+00 - val_loss: 1.6919\n",
      "Epoch 12/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6277 - val_accuracy: 0.0000e+00 - val_loss: 1.6911\n",
      "Epoch 13/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6341 - val_accuracy: 0.0000e+00 - val_loss: 1.6884\n",
      "Epoch 14/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6395 - val_accuracy: 0.0000e+00 - val_loss: 1.6825\n",
      "Epoch 15/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6279 - val_accuracy: 0.0000e+00 - val_loss: 1.6800\n",
      "Epoch 16/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6266 - val_accuracy: 0.0000e+00 - val_loss: 1.6783\n",
      "Epoch 17/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6282 - val_accuracy: 0.0000e+00 - val_loss: 1.6780\n",
      "Epoch 18/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6253 - val_accuracy: 0.0000e+00 - val_loss: 1.6749\n",
      "Epoch 19/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6201 - val_accuracy: 0.0000e+00 - val_loss: 1.6743\n",
      "Epoch 20/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6204 - val_accuracy: 0.0000e+00 - val_loss: 1.6734\n",
      "Epoch 21/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6135 - val_accuracy: 0.0000e+00 - val_loss: 1.6714\n",
      "Epoch 22/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6217 - val_accuracy: 0.0000e+00 - val_loss: 1.6709\n",
      "Epoch 23/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6184 - val_accuracy: 0.0000e+00 - val_loss: 1.6717\n",
      "Epoch 24/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 9.5181e-05 - loss: 0.6029 - val_accuracy: 0.0000e+00 - val_loss: 1.6701\n",
      "Epoch 25/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6254 - val_accuracy: 0.0000e+00 - val_loss: 1.6678\n",
      "Epoch 26/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6134 - val_accuracy: 0.0000e+00 - val_loss: 1.6665\n",
      "Epoch 27/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6145 - val_accuracy: 0.0000e+00 - val_loss: 1.6660\n",
      "Epoch 28/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6190 - val_accuracy: 0.0000e+00 - val_loss: 1.6640\n",
      "Epoch 29/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 2.0306e-04 - loss: 0.6126 - val_accuracy: 0.0000e+00 - val_loss: 1.6646\n",
      "Epoch 30/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 3.8717e-04 - loss: 0.6070 - val_accuracy: 0.0000e+00 - val_loss: 1.6637\n",
      "Epoch 31/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 5.0281e-04 - loss: 0.6103 - val_accuracy: 0.0000e+00 - val_loss: 1.6619\n",
      "Epoch 32/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6211 - val_accuracy: 0.0000e+00 - val_loss: 1.6633\n",
      "Epoch 33/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6088 - val_accuracy: 0.0000e+00 - val_loss: 1.6601\n",
      "Epoch 34/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 9.9997e-04 - loss: 0.6106 - val_accuracy: 0.0000e+00 - val_loss: 1.6602\n",
      "Epoch 35/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 6.3757e-04 - loss: 0.6037 - val_accuracy: 0.0000e+00 - val_loss: 1.6596\n",
      "Epoch 36/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 9.5523e-05 - loss: 0.6053 - val_accuracy: 0.0000e+00 - val_loss: 1.6581\n",
      "Epoch 37/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 7.2928e-04 - loss: 0.6075 - val_accuracy: 0.0000e+00 - val_loss: 1.6610\n",
      "Epoch 38/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5976 - val_accuracy: 0.0000e+00 - val_loss: 1.6590\n",
      "Epoch 39/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 9.6134e-04 - loss: 0.6116 - val_accuracy: 0.0000e+00 - val_loss: 1.6585\n"
     ]
    }
   ],
   "source": [
    "# 1. Tentukan dimensi input\n",
    "input_dim = X_combined.shape[1]\n",
    "\n",
    "# 2. Definisikan model Autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128)(input_layer)\n",
    "encoded = LeakyReLU(negative_slope=0.01)(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = Dense(64)(encoded)\n",
    "encoded = LeakyReLU(negative_slope=0.01)(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "decoded = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.01))(encoded)\n",
    "decoded = LeakyReLU(negative_slope=0.01)(decoded)\n",
    "decoded = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01))(decoded)\n",
    "decoded = LeakyReLU(negative_slope=0.01)(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# 3. Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error',  metrics=['accuracy'])\n",
    "\n",
    "# 4. Early stopping untuk menghindari overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 5. Train model\n",
    "history = autoencoder.fit(\n",
    "    X_combined, X_combined,  # Gunakan data gabungan\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 6. Simpan model\n",
    "autoencoder.save(\"coba.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f91e1f0-592b-48cc-8981-b8b834c90e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_learning_paths_with_encoder(data, encoder, user_preferences, programming_languages, github_username=None):\n",
    "    # Ambil skill dari GitHub jika tersedia\n",
    "    github_languages = get_github_skills(github_username) if github_username else []\n",
    "    combined_preferences = user_preferences + programming_languages + github_languages\n",
    "    \n",
    "    # Vectorize user preferences\n",
    "    user_vector = \" \".join(combined_preferences)\n",
    "    vectorizer = TfidfVectorizer(stop_words=indonesian_stopwords)\n",
    "    \n",
    "    # Gabungkan data learning path dan user preferences\n",
    "    all_vectors = vectorizer.fit_transform(data['Combined Summary'].tolist() + [user_vector])\n",
    "    user_vector_tfidf = all_vectors[-1].toarray()  # Make sure it's a 2D array\n",
    "    learning_paths_vectors = all_vectors[:-1].toarray()  # Make sure it's a 2D array\n",
    "    \n",
    "    # Check if the input shapes are correct\n",
    "    print(f\"Shape of user vector: {user_vector_tfidf.shape}\")\n",
    "    print(f\"Shape of learning paths vectors: {learning_paths_vectors.shape}\")\n",
    "    \n",
    "    # Representasi latent menggunakan encoder\n",
    "    latent_learning_paths = encoder.predict(learning_paths_vectors)\n",
    "    latent_user_vector = encoder.predict(user_vector_tfidf)\n",
    "    \n",
    "    # Hitung kemiripan antara representasi latent pengguna dan learning paths\n",
    "    similarity_scores = cosine_similarity(latent_user_vector, latent_learning_paths)\n",
    "    data['similarity'] = similarity_scores.flatten()\n",
    "\n",
    "    # Filter rekomendasi berdasarkan nilai kemiripan minimal 0.4\n",
    "    filtered_recommendations = data[data['similarity'] >= 0.4].sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # Jika ada kurang dari 5 yang memenuhi kriteria, ambil 5 teratas meskipun kurang dari 0.4\n",
    "    if len(filtered_recommendations) < 5:\n",
    "        recommendations = data.sort_values(by='similarity', ascending=False).drop_duplicates(subset=['Course Name_x']).head(5)\n",
    "    else:\n",
    "        recommendations = filtered_recommendations.drop_duplicates(subset=['Course Name_x']).head(5)\n",
    "\n",
    "    # Jelaskan alasan rekomendasi\n",
    "    explanations = []\n",
    "    for _, row in recommendations.iterrows():\n",
    "        explanations.append(\n",
    "            f\"Rekomendasi '{row['Course Name_x']}' memiliki kemiripan tinggi dengan preferensi Anda \"\n",
    "            f\"berdasarkan skill '{user_preferences}', bahasa pemrograman '{programming_languages}', dan aktivitas GitHub '{github_languages}'.\")\n",
    "\n",
    "    return recommendations[['Course Name_x', 'similarity']], explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cf8845-38cc-491d-9bac-a7b9e8d73df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user vector: (1, 414)\n",
      "Shape of learning paths vectors: (6039, 414)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_11_1/leaky_re_lu_28_1/LeakyRelu defined at (most recent call last):\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Temp\\ipykernel_4512\\1551639828.py\", line 7, in <module>\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Temp\\ipykernel_4512\\1807422840.py\", line 22, in recommend_learning_paths_with_encoder\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 510, in predict\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 208, in one_step_on_data_distributed\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 198, in one_step_on_data\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 96, in predict_step\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 899, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 584, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 899, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py\", line 57, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\activations\\activations.py\", line 120, in leaky_relu\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\nn.py\", line 294, in leaky_relu\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 50, in leaky_relu\n\nMatrix size-incompatible: In[0]: [32,414], In[1]: [411,128]\n\t [[{{node functional_11_1/leaky_re_lu_28_1/LeakyRelu}}]] [Op:__inference_one_step_on_data_distributed_74040]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m github_username \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mricomessi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Mendapatkan rekomendasi\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m recommendations, explanations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_learning_paths_with_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_preferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogramming_languages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgithub_username\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop Recommended Learning Paths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendations)\n",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m, in \u001b[0;36mrecommend_learning_paths_with_encoder\u001b[1;34m(data, encoder, user_preferences, programming_languages, github_username)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of learning paths vectors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_paths_vectors\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Representasi latent menggunakan encoder\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m latent_learning_paths \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_paths_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m latent_user_vector \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(user_vector_tfidf)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Hitung kemiripan antara representasi latent pengguna dan learning paths\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_11_1/leaky_re_lu_28_1/LeakyRelu defined at (most recent call last):\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Temp\\ipykernel_4512\\1551639828.py\", line 7, in <module>\n\n  File \"C:\\Users\\Rico Mesias\\AppData\\Local\\Temp\\ipykernel_4512\\1807422840.py\", line 22, in recommend_learning_paths_with_encoder\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 510, in predict\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 208, in one_step_on_data_distributed\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 198, in one_step_on_data\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 96, in predict_step\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 899, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 584, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 899, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py\", line 57, in call\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\activations\\activations.py\", line 120, in leaky_relu\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\ops\\nn.py\", line 294, in leaky_relu\n\n  File \"C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 50, in leaky_relu\n\nMatrix size-incompatible: In[0]: [32,414], In[1]: [411,128]\n\t [[{{node functional_11_1/leaky_re_lu_28_1/LeakyRelu}}]] [Op:__inference_one_step_on_data_distributed_74040]"
     ]
    }
   ],
   "source": [
    "# Contoh Penggunaan\n",
    "user_preferences = ['Backend Developer', 'cloud computing', 'API Processing']  # Skill yang ingin dikembangkan\n",
    "programming_languages = ['JavaScript', 'AWS', 'Java']  # Bahasa pemrograman yang diminati\n",
    "github_username = 'ricomessi'\n",
    "\n",
    "# Mendapatkan rekomendasi\n",
    "recommendations, explanations = recommend_learning_paths_with_encoder(data, encoder, user_preferences, programming_languages, github_username)\n",
    "\n",
    "print(\"\\nTop Recommended Learning Paths:\")\n",
    "print(recommendations)\n",
    "print(\"\\nExplanation for Recommendations:\")\n",
    "for explanation in explanations:\n",
    "    print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16a2ad0-2cab-403e-974b-a5808d55c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Recommended Learning Paths:\n",
      "                                          Course Name_x  similarity\n",
      "497   Cloud Practitioner Essentials (Belajar Dasar A...    0.167497\n",
      "799   Architecting on AWS (Membangun Arsitektur Clou...    0.125424\n",
      "3015               Belajar Dasar Pemrograman JavaScript    0.125308\n",
      "3224                      Menjadi Google Cloud Engineer    0.108695\n",
      "1112     Belajar Fundamental Back-End dengan JavaScript    0.108047\n",
      "\n",
      "Explanation for Recommendations:\n",
      "Rekomendasi 'Cloud Practitioner Essentials (Belajar Dasar AWS Cloud)' memiliki kemiripan tinggi dengan preferensi Anda berdasarkan skill '['Backend Developer', 'cloud computing', 'API Processing']', bahasa pemrograman '['JavaScript', 'AWS', 'Java']', dan aktivitas GitHub '['HTML', 'PHP', 'CSS', 'Python', 'Java', 'JavaScript', 'Hack']'.\n",
      "Rekomendasi 'Architecting on AWS (Membangun Arsitektur Cloud di AWS)' memiliki kemiripan tinggi dengan preferensi Anda berdasarkan skill '['Backend Developer', 'cloud computing', 'API Processing']', bahasa pemrograman '['JavaScript', 'AWS', 'Java']', dan aktivitas GitHub '['HTML', 'PHP', 'CSS', 'Python', 'Java', 'JavaScript', 'Hack']'.\n",
      "Rekomendasi 'Belajar Dasar Pemrograman JavaScript' memiliki kemiripan tinggi dengan preferensi Anda berdasarkan skill '['Backend Developer', 'cloud computing', 'API Processing']', bahasa pemrograman '['JavaScript', 'AWS', 'Java']', dan aktivitas GitHub '['HTML', 'PHP', 'CSS', 'Python', 'Java', 'JavaScript', 'Hack']'.\n",
      "Rekomendasi 'Menjadi Google Cloud Engineer' memiliki kemiripan tinggi dengan preferensi Anda berdasarkan skill '['Backend Developer', 'cloud computing', 'API Processing']', bahasa pemrograman '['JavaScript', 'AWS', 'Java']', dan aktivitas GitHub '['HTML', 'PHP', 'CSS', 'Python', 'Java', 'JavaScript', 'Hack']'.\n",
      "Rekomendasi 'Belajar Fundamental Back-End dengan JavaScript' memiliki kemiripan tinggi dengan preferensi Anda berdasarkan skill '['Backend Developer', 'cloud computing', 'API Processing']', bahasa pemrograman '['JavaScript', 'AWS', 'Java']', dan aktivitas GitHub '['HTML', 'PHP', 'CSS', 'Python', 'Java', 'JavaScript', 'Hack']'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_github_skills(username):\n",
    "    # Ambil bahasa pemrograman dominan dari profil GitHub\n",
    "    url = f\"https://api.github.com/users/ricomessi/repos\"\n",
    "    response = requests.get(url)\n",
    "    repos = response.json() if response.status_code == 200 else []\n",
    "\n",
    "    languages = []\n",
    "    for repo in repos:\n",
    "        language_url = repo.get('languages_url')\n",
    "        if language_url:\n",
    "            lang_response = requests.get(language_url)\n",
    "            lang_data = lang_response.json() if lang_response.status_code == 200 else {}\n",
    "            languages.extend(lang_data.keys())\n",
    "\n",
    "    return list(set(languages))  # Mengambil bahasa unik\n",
    "\n",
    "def recommend_learning_paths(data, user_preferences, programming_languages, github_username=None):\n",
    "    # Ambil skill dari GitHub jika tersedia\n",
    "    github_languages = get_github_skills(github_username) if github_username else []\n",
    "    combined_preferences = user_preferences + programming_languages + github_languages\n",
    "    \n",
    "    # Buat vector user preferences\n",
    "    user_vector = \" \".join(combined_preferences)\n",
    "    vectorizer = TfidfVectorizer(stop_words=indonesian_stopwords)\n",
    "    \n",
    "    # Gabungkan data learning path dan user preferences\n",
    "    all_vectors = vectorizer.fit_transform(data['Combined Summary'].tolist() + [user_vector])\n",
    "    user_vector_tfidf = all_vectors[-1]\n",
    "    learning_paths_vectors = all_vectors[:-1]\n",
    "    \n",
    "    # Hitung kemiripan antara user preferences dan learning paths\n",
    "    similarity_scores = cosine_similarity(user_vector_tfidf, learning_paths_vectors)\n",
    "    data['similarity'] = similarity_scores.flatten()\n",
    "\n",
    "    # Filter rekomendasi berdasarkan nilai kemiripan minimal 0.4\n",
    "    filtered_recommendations = data[data['similarity'] >= 0.4].sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # Jika ada kurang dari 5 yang memenuhi kriteria, ambil 5 teratas meskipun kurang dari 0.4\n",
    "    if len(filtered_recommendations) < 5:\n",
    "        recommendations = data.sort_values(by='similarity', ascending=False).drop_duplicates(subset=['Course Name_x']).head(5)\n",
    "    else:\n",
    "        recommendations = filtered_recommendations.drop_duplicates(subset=['Course Name_x']).head(5)\n",
    "\n",
    "    # Jelaskan alasan rekomendasi\n",
    "    explanations = []\n",
    "    for _, row in recommendations.iterrows():\n",
    "        explanations.append(\n",
    "            f\"Rekomendasi '{row['Course Name_x']}' memiliki kemiripan tinggi dengan preferensi Anda \"\n",
    "            f\"berdasarkan skill '{user_preferences}', bahasa pemrograman '{programming_languages}', dan aktivitas GitHub '{github_languages}'.\")\n",
    "\n",
    "    return recommendations[['Course Name_x', 'similarity']], explanations\n",
    "\n",
    "# Contoh Penggunaan\n",
    "user_preferences = ['Backend Developer', 'cloud computing', 'API Processing']  # Skill yang ingin dikembangkan\n",
    "programming_languages = ['JavaScript', 'AWS', 'Java']  # Bahasa pemrograman yang diminati\n",
    "github_username = 'username_github'\n",
    "\n",
    "# Mendapatkan rekomendasi\n",
    "recommendations, explanations = recommend_learning_paths(data, user_preferences, programming_languages, github_username)\n",
    "\n",
    "print(\"\\nTop Recommended Learning Paths:\")\n",
    "print(recommendations)\n",
    "print(\"\\nExplanation for Recommendations:\")\n",
    "for explanation in explanations:\n",
    "    print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf39fa3-2507-4006-b2c8-fabe6276e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation for Recommendations:\n",
      "Course 'Memulai Pemrograman dengan Python' is recommended because it has a high similarity score of 0.16 with your preferences.\n",
      "Course 'Belajar Pemrograman Prosedural dengan Python' is recommended because it has a high similarity score of 0.11 with your preferences.\n"
     ]
    }
   ],
   "source": [
    "def explain_recommendation(recommendations, data):\n",
    "    explanations = []\n",
    "    for idx, row in recommendations.iterrows():\n",
    "        course_name = row['Course Name_x']\n",
    "        similarity = row['similarity']\n",
    "        explanation = f\"Course '{course_name}' is recommended because it has a high similarity score of {similarity:.2f} with your preferences.\"\n",
    "        explanations.append(explanation)\n",
    "    return explanations\n",
    "\n",
    "# Menampilkan penjelasan\n",
    "explanations = explain_recommendation(recommendations, data)\n",
    "print(\"\\nExplanation for Recommendations:\")\n",
    "for explanation in explanations:\n",
    "    print(explanation)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d230bd-a257-476e-9030-208b338c1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Recommended Learning Paths:\n",
      "                           Course Name_x  similarity\n",
      "6038  Belajar Strategi Pengembangan Diri         0.0\n",
      "0      Memulai Pemrograman dengan Kotlin         0.0\n",
      "1      Memulai Pemrograman dengan Kotlin         0.0\n",
      "\n",
      "Explanation for Recommendations:\n",
      "Course 'Belajar Strategi Pengembangan Diri' is recommended because it has a high similarity score of 0.00 with your preferences.\n",
      "Course 'Memulai Pemrograman dengan Kotlin' is recommended because it has a high similarity score of 0.00 with your preferences.\n",
      "Course 'Memulai Pemrograman dengan Kotlin' is recommended because it has a high similarity score of 0.00 with your preferences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rico Mesias\\OneDrive\\Documents\\VsCode\\DicodingCompany\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model yang telah disimpan\n",
    "model = load_model(\"learning_path_recommendation_model.h5\")\n",
    "\n",
    "# Contoh input pengguna\n",
    "user_preferences = [\"PHP\", 'CodeIgniter', 'Java']  # Skill dan minat\n",
    "recommendations = recommend_learning_paths(data, user_preferences)\n",
    "\n",
    "print(\"\\nTop 3 Recommended Learning Paths:\")\n",
    "print(recommendations)\n",
    "\n",
    "# Penjelasan mengapa course ini direkomendasikan\n",
    "explanations = explain_recommendation(recommendations, data)\n",
    "print(\"\\nExplanation for Recommendations:\")\n",
    "for explanation in explanations:\n",
    "    print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4697c-bd00-454c-97c0-5eb4f81d8b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
